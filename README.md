# ğŸ” Lightweight LLM Access API with FastAPI & Ollama

This is a simple yet functional Python-based API to **control access to a local LLM (Large Language Model)** using API key authentication and credit limits. Built with **FastAPI** and **Ollama**, this project lets you limit usage per user and serve AI responses securely from a local model.

## ğŸš€ Features

- ğŸŒ REST API to interact with a local AI model
- ğŸ”‘ API key-based authentication
- ğŸ’³ Token/credit system for usage control
- ğŸ§  Works with `ollama`-hosted models like `mistral`
- ğŸ“¦ Lightweight and dependency-minimal

---

## ğŸ› ï¸ Requirements

Make sure Python 3.9+ is installed, then run:

```bash
pip install -r requirements.txt
```

## â–¶ï¸ How to Run
Start the API:

```bash
python -m uvicorn main:app --reload
```

Use test.py to make a test call:
```bash
python test.py
```
